{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rwH4faaRpyop"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LayerNormalization\n",
    "from tensorflow.keras.layers import LSTM, SpatialDropout1D, GRU, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Flatten, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Root Path (Where model and results will be stored and where model will be loaded from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = './'\n",
    "\n",
    "MODEL_INDEX = 1\n",
    "EMBEDDING_VECTOR_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ROOT_PATH + 'Models'):\n",
    "    os.makedirs(ROOT_PATH + 'Models')\n",
    "\n",
    "if not os.path.exists(ROOT_PATH + 'Results'):\n",
    "    os.makedirs(ROOT_PATH + 'Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PVjPCuylpyow"
   },
   "outputs": [],
   "source": [
    "def load_data(path, name_of_data):\n",
    "    data, labels = [], []\n",
    "    \n",
    "    if name_of_data == \"test\":\n",
    "        \n",
    "        with open(path + 'data/' + name_of_data + '.txt') as f1:\n",
    "            reader1 = csv.reader(f1, delimiter = \"\\n\")\n",
    "            for line in reader1:\n",
    "                data.append(line[0])\n",
    "                \n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        with open(path + 'data/' + name_of_data + '.txt') as f1:\n",
    "            reader1 = csv.reader(f1, delimiter = \"\\n\")\n",
    "            for line in reader1:\n",
    "                data.append(line[0])\n",
    "\n",
    "        with open(path + 'data/' + name_of_data +'.labels') as f2:\n",
    "            reader2 = csv.reader(f2)\n",
    "            for line in reader2:\n",
    "                labels.append(int(line[0]))\n",
    "\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TUQowZf5pyox"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(ROOT_PATH, \"train\")\n",
    "X_val, y_val = load_data(ROOT_PATH, \"val\")\n",
    "X_test = load_data(ROOT_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "501M8XXypyoy",
    "outputId": "9607f451-16ef-4fda-bf7a-e11f0138cde5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Records:  41816 41816\n",
      "Val Records:  5227 5227\n",
      "Test Records:  5227\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Records: \", len(X_train), len(y_train))\n",
    "print(\"Val Records: \", len(X_val), len(y_val))\n",
    "print(\"Test Records: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample From Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-6GGOVkpyoz",
    "outputId": "61f4cff3-035e-441b-e7b4-e0fcd927404a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grease a baking sheet and line with parchment paper. Mix white chocolate chips and peanut butter together in a microwave-safe bowl; heat in microwave until half-melted, 30 seconds to 1 minutes. Stir. Place semi-sweet chocolate chips in a microwave-safe bowl; heat in microwave until half-melted, 15 to 30 seconds. Stir vanilla extract into half-melted semi-sweet chocolate. Spread peanut butter mixture onto the prepared baking sheet. Evenly distribute melted semi-sweet chocolate over peanut butter mixture. Using the tip of a sharp knife, drag semi-sweet chocolate through peanut butter mixture making a marble-pattern. Refrigerate until set, 30 minutes to 2 hours. Cut into pieces and store in an air-tight container. I just recently tried this recipe, and everyone in the family loved it! They call it Reese'sÂ® Peanut Butter Bark! It's very good!\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Training dataset is tokenized (words turned to integers) for input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1W3vrzxMpyoz",
    "outputId": "836569a7-246f-43ee-cc15-971001912646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size  19733\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, lower = True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size \", vocab_size)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0cQTB4tpyoz",
    "outputId": "d8519568-8a38-43e7-c8ec-43e35c42acc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153, 3, 35, 130, 2, 346, 6, 560, 198, 33, 168, 120, 250, 2, 389, 40, 39, 4, 3, 344, 699, 19, 12, 4, 344, 7, 151, 170, 116, 476, 5, 17, 9, 14, 31, 2049, 210, 120, 250, 4, 3, 344, 699, 19, 12, 4, 344, 7, 151, 170, 119, 5, 116, 476, 14, 131, 276, 11, 151, 170, 2049, 210, 120, 121, 389, 40, 18, 147, 1, 128, 35, 130, 169, 1723, 170, 2049, 210, 120, 16, 389, 40, 18, 199, 1, 1299, 8, 3, 967, 481, 5848, 2049, 210, 120, 183, 389, 40, 18, 548, 3, 3126, 1866, 175, 7, 84, 116, 9, 5, 26, 127, 145, 11, 227, 2, 464, 4, 72, 980, 1179, 496, 54, 140, 2401, 1032, 23, 74, 2, 656, 4, 1, 242, 964, 34, 221, 1734, 34, 7706, 389, 40, 3077, 208, 165, 252]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are either truncated or padded so that all sequences are of equal length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ukqZi8ULpyo0"
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_review_length, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YSR-9xkzpyo0"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejecUvWpyo0",
    "outputId": "9fcb0dec-5aa5-4bce-869f-d2a79224ce00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "int32\n",
      "int32\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtype)\n",
    "print(X_val.dtype)\n",
    "print(X_test.dtype)\n",
    "\n",
    "print(y_train.dtype)\n",
    "print(y_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are changed to 0 to 11 (for 12 categories) as the model labels start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LcC9xZetpyo2"
   },
   "outputs": [],
   "source": [
    "y_train = y_train - 1\n",
    "y_val = y_val - 1\n",
    "\n",
    "train_labels = to_categorical(y_train, num_classes = 12)\n",
    "val_labels = to_categorical(y_val, num_classes = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onciq84Xpyo2",
    "outputId": "11bc8abf-e78d-4e80-85d2-c331810c89e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_YmwGD6pyo2",
    "outputId": "5825b12e-dd1e-4e4e-d832-77bc366d4f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41816, 1000) (41816, 12)\n",
      "(5227, 1000) (5227, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, train_labels.shape)\n",
    "print(X_val.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding file is imported for use in the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-vN3fAVQ-13",
    "outputId": "2a5cf704-e873-4217-c124-1805d926af07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 50d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(ROOT_PATH + 'glove.6B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 50d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB-vorhQQ_gD",
    "outputId": "c9e65e22-1bf0-4df6-860b-6c63f4ef6b86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 19732\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_VECTOR_LENGTH))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS_HaixVpyo3",
    "outputId": "e237260c-a97b-47ed-950f-808123516f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_input (InputLayer)    [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1_input (InputLayer)  [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1000, 50)     986650      embedding_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 50)     986650      embedding_1_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000, 50)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1000, 100)    30600       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1000, 100)    30600       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100)          45600       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100)          45600       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          12928       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          12928       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           3084        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,154,640\n",
      "Trainable params: 2,154,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "837/837 [==============================] - 260s 264ms/step - loss: 1.5945 - accuracy: 0.4583 - val_loss: 0.7652 - val_accuracy: 0.7375\n",
      "Epoch 2/100\n",
      "837/837 [==============================] - 221s 264ms/step - loss: 0.7283 - accuracy: 0.7510 - val_loss: 0.5608 - val_accuracy: 0.8045\n",
      "Epoch 3/100\n",
      "837/837 [==============================] - 222s 265ms/step - loss: 0.5394 - accuracy: 0.8176 - val_loss: 0.4856 - val_accuracy: 0.8334\n",
      "Epoch 4/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.4411 - accuracy: 0.8481 - val_loss: 0.4461 - val_accuracy: 0.8433\n",
      "Epoch 5/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.3832 - accuracy: 0.8673 - val_loss: 0.4062 - val_accuracy: 0.8651\n",
      "Epoch 6/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.3265 - accuracy: 0.8864 - val_loss: 0.3989 - val_accuracy: 0.8645\n",
      "Epoch 7/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.3004 - accuracy: 0.8967 - val_loss: 0.3689 - val_accuracy: 0.8772\n",
      "Epoch 8/100\n",
      "837/837 [==============================] - 224s 267ms/step - loss: 0.2692 - accuracy: 0.9057 - val_loss: 0.3584 - val_accuracy: 0.8808\n",
      "Epoch 9/100\n",
      "837/837 [==============================] - 223s 267ms/step - loss: 0.2354 - accuracy: 0.9176 - val_loss: 0.3603 - val_accuracy: 0.8867\n",
      "Epoch 10/100\n",
      "837/837 [==============================] - 224s 267ms/step - loss: 0.2089 - accuracy: 0.9283 - val_loss: 0.3666 - val_accuracy: 0.8896\n",
      "Epoch 11/100\n",
      "837/837 [==============================] - 224s 267ms/step - loss: 0.1927 - accuracy: 0.9331 - val_loss: 0.3568 - val_accuracy: 0.8934\n",
      "Epoch 12/100\n",
      "837/837 [==============================] - 224s 267ms/step - loss: 0.1723 - accuracy: 0.9369 - val_loss: 0.3670 - val_accuracy: 0.9001\n",
      "Epoch 13/100\n",
      "837/837 [==============================] - 223s 267ms/step - loss: 0.1573 - accuracy: 0.9452 - val_loss: 0.3722 - val_accuracy: 0.8975\n",
      "Epoch 14/100\n",
      "837/837 [==============================] - 223s 267ms/step - loss: 0.1374 - accuracy: 0.9531 - val_loss: 0.3742 - val_accuracy: 0.9011\n",
      "Epoch 15/100\n",
      "837/837 [==============================] - 223s 267ms/step - loss: 0.1311 - accuracy: 0.9539 - val_loss: 0.3802 - val_accuracy: 0.9028\n",
      "Epoch 16/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.1162 - accuracy: 0.9577 - val_loss: 0.4259 - val_accuracy: 0.8990\n",
      "Epoch 17/100\n",
      "837/837 [==============================] - 222s 266ms/step - loss: 0.1005 - accuracy: 0.9637 - val_loss: 0.4144 - val_accuracy: 0.9001\n",
      "Epoch 18/100\n",
      "837/837 [==============================] - 223s 266ms/step - loss: 0.0965 - accuracy: 0.9649 - val_loss: 0.4445 - val_accuracy: 0.8976\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd49dec8050>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Model\n",
    "\n",
    "####\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Embedding(vocab_size, EMBEDDING_VECTOR_LENGTH, \n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = max_review_length, \n",
    "                    trainable=True))\n",
    "\n",
    "left_branch.add(Dropout(0.3))\n",
    "\n",
    "left_branch.add(Bidirectional(GRU(EMBEDDING_VECTOR_LENGTH, return_sequences = True)))\n",
    "left_branch.add(Bidirectional(GRU(EMBEDDING_VECTOR_LENGTH)))\n",
    "left_branch.add(Dense(128, activation='relu'))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Embedding(vocab_size, EMBEDDING_VECTOR_LENGTH, \n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length = max_review_length, \n",
    "                    trainable=True))\n",
    "\n",
    "right_branch.add(Dropout(0.3))\n",
    "\n",
    "right_branch.add(Bidirectional(GRU(EMBEDDING_VECTOR_LENGTH, return_sequences = True)))\n",
    "right_branch.add(Bidirectional(GRU(EMBEDDING_VECTOR_LENGTH)))\n",
    "right_branch.add(Dense(128, activation='relu'))\n",
    "\n",
    "merged = concatenate([left_branch.output, right_branch.output])\n",
    "\n",
    "output_layer = Dense(12, activation='softmax')(merged)\n",
    "\n",
    "final_model = Model(inputs = [left_branch.input, right_branch.input], outputs = [output_layer])\n",
    "final_model.summary()\n",
    "\n",
    "####\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "final_model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelname = 'PR2_' + str(MODEL_INDEX)\n",
    "model_json = final_model.to_json()\n",
    "\n",
    "# where the model will be saved \n",
    "with open(ROOT_PATH + 'Models/_' + modelname + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# define early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', \n",
    "                          min_delta=0.001, \n",
    "                          patience=7, \n",
    "                          verbose=2, \n",
    "                          mode='auto', \n",
    "                          baseline=None, \n",
    "                          restore_best_weights=True)  \n",
    "\n",
    "# define modelcheckpoint callback\n",
    "checkpointer = ModelCheckpoint(filepath = ROOT_PATH + 'Models/_' + modelname + '.hdf5',\n",
    "                               monitor='val_loss', \n",
    "                               save_best_only=True)\n",
    "\n",
    "# callbacks list\n",
    "callbacks_list = [earlystop, \n",
    "                  checkpointer,\n",
    "                  ReduceLROnPlateau()]\n",
    "\n",
    "# Training of the model\n",
    "final_model.fit([X_train, X_train], train_labels, \n",
    "          validation_data=([X_val, X_val], val_labels), \n",
    "          epochs=100, \n",
    "          batch_size=50, \n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INDEX = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PR2_31 from disk\n"
     ]
    }
   ],
   "source": [
    "modelname='PR2_' + str(MODEL_INDEX)\n",
    "\n",
    "json_file = open(ROOT_PATH + 'Models/_' + modelname + '.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(filepath = ROOT_PATH + 'Models/_' + modelname + '.hdf5')\n",
    "print(\"Loaded \" + modelname +\" from disk\")\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer = opt, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Done\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = loaded_model.predict([X_test,X_test])\n",
    "print(\"Prediction Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5227,)\n",
      "File saved at  ./Results/PR2_31_Final_Results.txt\n"
     ]
    }
   ],
   "source": [
    "final_predictions = np.argmax(predicted_labels, axis=-1)\n",
    "print(final_predictions.shape)\n",
    "final_predictions = final_predictions + 1\n",
    "\n",
    "with open(ROOT_PATH + \"Results/\" + modelname + \"_Final_Results.txt\", 'w', newline='') as w1:\n",
    "\n",
    "    writer = csv.writer(w1, delimiter=' ')\n",
    "\n",
    "    for p in list(final_predictions):\n",
    "        writer.writerow([p])\n",
    "        \n",
    "print(\"File saved at \", ROOT_PATH + \"Results/\" + modelname + \"_Final_Results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe 1\n",
      "Bring a large pot of water to a boil. Add potatoes, and cook until tender but still firm, 12 to 15 minutes; drain. Place bacon in a large, deep skillet. Cook over medium high heat until evenly brown. Cut into small chunks; set aside. Place potatoes into skillet, and cook on medium heat until browned. Flip potatoes occasionally to prevent sticking. Stir in green pepper, red pepper, onion, and mushrooms. Cook until vegetables are tender. Stir in cooked bacon, and season with salt and pepper. Cover with shredded cheese, and turn mixture until cheese is melted. Keep on low heat while cooking eggs. Cook eggs to your preferred style. Place potatoes in a large serving dish, and top with eggs (2 per serving). Hearty breakfast skillets. Serve with toast or muffins.\n",
      "\n",
      "Predicted category: 3\n"
     ]
    }
   ],
   "source": [
    "test_data = load_data(ROOT_PATH, \"test\")\n",
    "\n",
    "print(\"Recipe 1\")\n",
    "print(test_data[0])\n",
    "\n",
    "print()\n",
    "print(\"Predicted category:\", final_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PR2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
